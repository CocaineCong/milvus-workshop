{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3.1 Milvus 在图片搜索中的应用\n",
   "id": "79d2b397612e89bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Milvus 在 RAG（Retrieval-Augmented Generation，检索增强生成）系统中，扮演着“外部知识库”的核心角色。它的主要任务是将经过处理的文档内容转化为高维语义向量，并高效地进行存储与检索。\n",
    "\n",
    "具体来说，RAG 系统会先将原始文档分成多个文本块，并利用嵌入模型将每个文本块转化为一个语义向量。Milvus 作为向量数据库，将这些文本块连同它们的向量及元数据一同保存。当用户提出问题时，系统会将问题同样转化为向量，并通过 Milvus 快速检索出最相关的文本块。\n",
    "\n",
    "这些被检索到的文本块，作为知识依据和上下文，被输入到大语言模型中，用于进一步生成符合实际、内容可靠的答案。得益于 Milvus 的高性能向量检索能力，即便面对百万甚至亿级的文档数据，RAG 系统也能在极短时间内给出精准的知识响应。\n",
    "\n",
    "简而言之，Milvus 是 RAG 体系中的“知识中枢”，保障了外部知识的有效扩展与按需调用。它让大模型生成不再依赖记忆和猜测，而是基于真实的、可追溯的语义资料。这也是现代智能问答系统可信、专业和可扩展的基础之一。\n",
    "\n"
   ],
   "id": "50eeca004ff79c75"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "在本章中，我们将系统性回顾 RAG（Retrieval-Augmented Generation，检索增强生成）的核心技术流程，帮助大家从整体上理解这一技术架构是如何工作的。在后续的实验中，我们将基于这些环节逐步搭建一个具备真实问答能力的智能系统。\n",
    "\n",
    "RAG 的提出，源于传统大语言模型（LLM）存在的两大问题：\n",
    "\n",
    "1. **记忆范围有限**：模型上下文窗口有限，无法直接处理大体量知识。\n",
    "2. **容易“胡说”**：模型会根据语义猜测输出内容，可能编造事实（Hallucination）。\n",
    "\n",
    "而 RAG 的核心思想是：\\*\\*将检索模块引入生成过程，让模型在生成答案前先“查资料”。\\*\\*这样可以大幅提升回答的相关性和准确性，尤其适用于企业文档问答、知识库助手、法律法规摘要、技术支持等场景。\n",
    "\n",
    "RAG 系统通常包含以下六个关键步骤：\n",
    "\n",
    "```\n",
    "文档加载 → 分块（Chunking）→ 向量化（Embedding）→ 存储 → 检索 → 生成\n",
    "```\n",
    "\n",
    "下面我们逐一简要介绍这六个步骤的意义与要点。\n",
    "\n",
    "---\n",
    "\n",
    "### 1️⃣ 文档加载（Document Loading）\n",
    "\n",
    "这是整个流程的起点。我们需要先将企业内部的知识内容加载进系统中。常见的文档格式包括 `.pdf`、`.txt`、`.md`、`.docx` 等，信息来源可以是本地文件、网页、数据库，甚至是云端文档（如 Notion、Confluence、S3 等）。\n",
    "\n",
    "在实践中，推荐使用 LangChain 中的 `DocumentLoader` 工具，结合 `Unstructured`、`PyMuPDF`、`pdfplumber` 等库，帮助我们将文档加载为标准的文本数据结构（Document List）。\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ 分块（Chunking）\n",
    "\n",
    "原始文档往往篇幅较长，不适合直接向量化，因此需要进行“分块”处理，将长文档拆解为数百字的短文本块（chunk）。\n",
    "\n",
    "常见策略包括：\n",
    "\n",
    "* 固定大小拆分（如每 500 字一个 chunk）\n",
    "* 滑动窗口拆分（如每 500 字，重叠 100 字）\n",
    "* 按段落或标题拆分（保留语义完整性）\n",
    "\n",
    "Chunking 的策略将直接影响后续检索效果：块太小语义不完整，块太大可能模糊焦点。因此，chunk size 和 overlap 是需要调优的重要参数。\n",
    "\n",
    "---\n",
    "\n",
    "### 3️⃣ 向量化（Embedding）\n",
    "\n",
    "每个文本块需要通过语言模型转化为向量，这个向量就是模型所理解的“语义表示”。\n",
    "\n",
    "常用的嵌入模型包括：\n",
    "\n",
    "* OpenAI 的 `text-embedding-3-small`\n",
    "* Sentence Transformers 的 `all-MiniLM-L6-v2`\n",
    "* Amazon Bedrock 提供的 `Titan Embeddings`\n",
    "* Cohere 的 `embed-english-light-v3`\n",
    "\n",
    "生成的向量通常是 384、768 或 1536 维的浮点数组，表示该文本块的语义位置。后续我们可以用这些向量做相似度比较（如余弦相似度）来实现语义检索。\n",
    "\n",
    "---\n",
    "\n",
    "### 4️⃣ 存储（Storage）\n",
    "\n",
    "生成的向量需要持久化保存，以供随时检索。我们通常使用**向量数据库**来完成这项任务。\n",
    "\n",
    "主流的向量数据库包括：\n",
    "\n",
    "* **Milvus**（支持亿级数据，性能强）\n",
    "* FAISS（轻量高效，适合本地测试）\n",
    "* Pinecone（托管型，简单易用）\n",
    "* Weaviate（支持 GraphQL 和语义推理）\n",
    "\n",
    "这些数据库不仅能保存向量本身，还能附带原始文本和元数据（如文档名、段落位置等），方便在回答中进行引用。\n",
    "\n",
    "---\n",
    "\n",
    "### 5️⃣ 检索（Retrieval）\n",
    "\n",
    "当用户提出问题时，系统会先将问题进行向量化，然后与数据库中的向量进行相似度搜索，找出最相关的文本块。这个过程就是“语义检索”。\n",
    "\n",
    "为了提升检索准确性，很多系统还会使用混合检索（Hybrid Search）技术，结合关键词匹配（如 BM25）与语义匹配，或加入 metadata filter 进行限制筛选。\n",
    "\n",
    "检索结果通常返回 Top-K（如最相关的 3\\~5 段）文本块，作为生成模块的输入上下文。\n",
    "\n",
    "---\n",
    "\n",
    "### 6️⃣ 生成（Generation）\n",
    "\n",
    "最后，将检索到的文本块与用户提问一并输入到大语言模型中，由模型基于上下文生成最终答案。\n",
    "\n",
    "你可以自定义 prompt 模板，如：\n",
    "\n",
    "```\n",
    "请根据以下内容回答问题：\n",
    "{context}\n",
    "\n",
    "问题：{question}\n",
    "```\n",
    "\n",
    "LLM 会参考检索内容进行生成，从而提升准确性、降低幻觉率。\n",
    "\n",
    "---\n",
    "\n",
    "通过这六个步骤，我们构建出了一个完整的“从知识库中查找信息并生成回答”的智能问答系统。接下来的实验中，我们将以这一流程为基础，实战搭建一个基于 LangChain 和 Milvus 的 RAG 系统。"
   ],
   "id": "c02ad79156d99476"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "好的，以下是完整约 1000 字的章节内容，围绕「Milvus 在 RAG 中的角色」，适合作为实验手册中的一章开头，语言清晰、结构合理、技术导向明确：\n",
    "\n",
    "---\n",
    "\n",
    "## Milvus 在 RAG 中的角色\n",
    "\n",
    "在构建一个基于 RAG（Retrieval-Augmented Generation，检索增强生成）的问答系统时，很多人第一时间关注的是大语言模型（LLM）的回答能力，然而，真正决定答案质量的往往是模型“看到了什么”。这些“上下文信息”的提供，正是向量数据库的任务。而在众多向量数据库中，**Milvus** 是最常被使用、也最适合工程化落地的一种解决方案。\n",
    "\n",
    "本章将带大家深入了解：Milvus 在整个 RAG 架构中究竟承担了怎样的角色？它是如何提升问答系统的准确性、效率与可扩展性的？我们又该如何正确地使用它？\n",
    "\n",
    "---\n",
    "\n",
    "### 🧩 Milvus 是什么？\n",
    "\n",
    "Milvus 是一个开源、高性能的向量数据库系统，专门用于处理大规模的向量数据。与传统的关系型数据库不同，Milvus 不擅长做精确查找，而是专注于语义检索——也就是**给出一个查询向量，找出与之“语义相似”的数据项**。\n",
    "\n",
    "在 NLP 场景中，我们将文本通过嵌入模型转化为向量表示，这些向量就代表了文本的语义。Milvus 能够快速地在成千上万个文本向量中，找出与某个问题向量最接近的几条内容，从而实现真正的“先查资料，再作答”。\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 在 RAG 流程中的职责\n",
    "\n",
    "Milvus 在 RAG 系统中主要承担两项关键任务：\n",
    "\n",
    "#### 1. 存储文本块及其语义向量\n",
    "\n",
    "RAG 系统的第一步是加载文档，并将其分成若干小段（chunk）。每一段文本都会被一个嵌入模型（如 `all-MiniLM-L6-v2` 或 OpenAI Embedding 模型）转化为一个高维向量。\n",
    "\n",
    "这些向量不能直接存在内存中，否则随着文档数量增长，系统将无法负载。此时，Milvus 作为向量数据库登场，**它不仅存储这些向量，还一并保存原始文本和元数据（如文件名、页码、段落编号等）**，形成一个完整的语义检索库。\n",
    "\n",
    "#### 2. 快速检索与用户问题相似的文本块\n",
    "\n",
    "当用户提出一个自然语言问题时，该问题也会被同样的嵌入模型转化为一个语义向量。这个查询向量会被发送给 Milvus，系统会在存储库中查找与之最接近的 Top-K 向量所对应的文本块。\n",
    "\n",
    "这一步检索结果将作为**上下文**被喂给 LLM（如 GPT、Claude），成为其回答问题的依据。换句话说，Milvus 决定了 LLM“看到了什么”，而 LLM 决定了“怎么说”。\n",
    "\n",
    "---\n",
    "\n",
    "### ⚡ 为什么选择 Milvus？\n",
    "\n",
    "虽然市面上有许多向量数据库，比如 FAISS、Pinecone、Weaviate 等，但 Milvus 拥有一系列特点，使其特别适合构建工业级的 RAG 系统：\n",
    "\n",
    "* **高性能**：基于 FAISS/HNSW/IVF 等高效检索算法，支持亿级向量的快速查询。\n",
    "* **结构化过滤能力**：支持向量检索 + metadata（结构化标签）联合查询，比如“只检索标签为技术文档的内容”。\n",
    "* **部署灵活**：支持本地部署（Docker/K8s）与云托管（Zilliz Cloud）。\n",
    "* **与 LangChain 深度集成**：可作为 `VectorStore`，直接参与 RAG 工作流。\n",
    "* **活跃社区支持**：拥有完善文档、示例项目和 API，学习成本低。\n",
    "\n",
    "这些特性使得 Milvus 不仅适合初学者实验项目，也可以轻松扩展到企业级应用场景。\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 总结：Milvus 是 RAG 的“记忆中枢”\n",
    "\n",
    "在 RAG 的六步流程中：\n",
    "\n",
    "```\n",
    "文档加载 → 分块（Chunking）→ 向量化（Embedding）→ 存储（Milvus）→ 检索 → 生成\n",
    "```\n",
    "\n",
    "Milvus 虽不是回答问题的“主角”，却是整个系统的“记忆中枢”。它让大语言模型具备了“短期记忆之外的长期记忆”，从而能够处理结构化文档、非结构化知识、复杂报告等超出上下文窗口的海量信息。\n",
    "\n",
    "RAG 的本质是连接检索与生成，而 Milvus 就是这一连接中的桥梁，它保证了系统的可扩展性、准确性与实时性。\n",
    "\n",
    "---\n",
    "\n",
    "在接下来的章节中，我们将正式上手：使用 LangChain 将向量数据写入 Milvus，并完成一次真实的问题检索与上下文生成流程。a\n"
   ],
   "id": "8b2c2b741fc53d1a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# RAG with LangChain & Milvus: Sentence Transformers 本地模型 Demo\n",
    "\n",
    "> 适合无 OpenAI Key、企业内网、敏感知识库等场景。\n",
    "\n",
    "**依赖安装**\n",
    "```bash\n",
    "pip install langchain pymilvus sentence-transformers ollama\n",
    "```\n",
    "Milvus 推荐 docker 一键启动，\n",
    "文档示例路径：docs/enterprise_guide.txt\n"
   ],
   "id": "841c53f8aa1d657d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
